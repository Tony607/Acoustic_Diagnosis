{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify motor types by sound\n",
    "In production environment like a warehouse distribution center, there are hundreds of AC motors that are driving conveyor belts and sorters day and night. Letâ€™s say one motor at a critical link breaks down, it can cause major downtime of the whole system. An experienced maintenance engineer can identify a faulty motor by listening to its sounds and take action to correct it before it is too late. \n",
    "\n",
    "In this demo, we are going to train our model to be an expert. It can tell if a motor is faulty by listening to its sound.\n",
    "\n",
    "Using MFCC features as model input, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC, [MFC wiki](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the wav files\n",
    "\n",
    "### Signal recording location\n",
    "- de: motor drive end\n",
    "- re: motor rear end\n",
    "\n",
    "### Type of motors\n",
    "- new: a new motor\n",
    "- used: a used motor, still functional\n",
    "- red: an identified bad motor\n",
    "\n",
    "e.g. **used_60Hz_re_10s_22khz.wav** means, \n",
    "- an used motor\n",
    "- VFD frequency is 60Hz\n",
    "- microphone attached to the rear end of the motor housing\n",
    "- recorded for 10 seconds\n",
    "- the sampling rate is 22Khz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_dir = \"./data/\"\n",
    "sound_file_paths = [\"new_60Hz_de_10s_22khz.wav\",\"new_60Hz_re_10s_22khz.wav\",\n",
    "                    \"used_60Hz_de_10s_22khz.wav\",\"used_60Hz_re_10s_22khz.wav\",\n",
    "                    \"red_60Hz_de_10s_22khz.wav\",\"red_60Hz_re_10s_22khz.wav\"]\n",
    "# Output tags\n",
    "sound_names = [\"new\",\"new\",\n",
    "               \"used\",\"used\",\n",
    "               \"red\",\"red\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to extract MFCC feature out of WAV files for model input\n",
    "\n",
    "- frames: 41, one frame consists of 512 samples\n",
    "- hop length: 512 is, the number of samples between successive **frames**\n",
    "- window_size: 512 * (41-1) = 20480. Total samples to compute the MFCCs features. Given sampling rate 22kHz, window time about 1 second.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_normalize(dataset):\n",
    "    mu = np.mean(dataset, axis=0)\n",
    "    sigma = np.std(dataset, axis=0)\n",
    "    return (dataset - mu) / sigma\n",
    "def windows(data, window_size):\n",
    "    start = 0\n",
    "    while start < len(data):\n",
    "        yield int(start), int(start + window_size)\n",
    "        start += (window_size / 2) # stepping at half window size\n",
    "def extract_features(base_dir, sound_file_paths, sound_names ,bands = 20, frames = 41):\n",
    "    window_size = 512 * (frames - 1)\n",
    "    mfccs = []\n",
    "    labels = []\n",
    "    for i, sound_file_path in enumerate(sound_file_paths):\n",
    "        sound_file_full_path = os.path.join(base_dir, sound_file_path)\n",
    "        sound_clip,s = librosa.load(sound_file_full_path)\n",
    "        sound_clip = feature_normalize(sound_clip)\n",
    "        label = sound_names[i]\n",
    "        for (start,end) in windows(sound_clip,window_size):\n",
    "            if(len(sound_clip[start:end]) == window_size):\n",
    "                signal = sound_clip[start:end]\n",
    "                # y: audio time series, sr: sampling rate, n_mfcc: number of MFCCs to return\n",
    "                # librosa.feature.mfcc() function return numpy array with shape (bands, frames)\n",
    "                # transpose since the model expects time axis(frames) come first\n",
    "                mfcc = librosa.feature.mfcc(y=signal, sr=s, n_mfcc = bands).T \n",
    "                mfccs.append(mfcc)\n",
    "                labels.append(label)\n",
    "    features = np.asarray(mfccs)\n",
    "    return np.array(features), np.array(labels,dtype = np.str)\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    return np.asarray(pd.get_dummies(labels), dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bands = 20\n",
    "frames = 41\n",
    "features,labels = extract_features(base_dir, sound_file_paths, sound_names, bands = bands, frames = frames)\n",
    "labels = one_hot_encode(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape of **features**\n",
    "(number of samples,frames,bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 41, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_iters = 300\n",
    "batch_size = 5\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_input = bands \n",
    "n_steps = frames\n",
    "n_hidden = 64\n",
    "n_classes = 3 # new , other , red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "weight = tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "paras\n",
    "- x: The RNN inputs for **tf.nn.dynamic_rnn**.\n",
    "- weight: softmax fully connected layer weight\n",
    "- bias: softmax fully connected layer bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell(lstm_size, keep_prob):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\n",
    "def RNN(x, weight, bias, keep_prob, num_layers=2):\n",
    "    lstm_layers = tf.contrib.rnn.MultiRNNCell([lstm_cell(n_hidden, keep_prob) for _ in range(num_layers)])\n",
    "    output, state = tf.nn.dynamic_rnn(lstm_layers, x, dtype = tf.float32)\n",
    "    output = tf.transpose(output, [1, 0, 2])\n",
    "    last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "    return tf.nn.softmax(tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hasee\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "prediction = RNN(x, weight, bias, keep_prob)\n",
    "# Define loss and optimizer\n",
    "loss_f = -tf.reduce_sum(y * tf.log(prediction))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss_f)\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss= 2.890719, Training Accuracy= 0.80000\n",
      "Iter 100, Minibatch Loss= 0.000040, Training Accuracy= 1.00000\n",
      "Iter 200, Minibatch Loss= 0.000072, Training Accuracy= 1.00000\n",
      "Test accuracy:  1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./model/acoustic.ckpt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "session=tf.InteractiveSession()\n",
    "# Initializing the variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for itr in range(training_iters):    \n",
    "    offset = (itr * batch_size) % (labels.shape[0] - batch_size)\n",
    "    batch_x = X_train[offset:(offset + batch_size), :, :]\n",
    "    batch_y = y_train[offset:(offset + batch_size), :]\n",
    "    _, c = session.run([optimizer, loss_f],feed_dict={x: batch_x, y : batch_y, keep_prob: 0.95})\n",
    "\n",
    "    if itr % display_step == 0:\n",
    "        # Calculate batch accuracy\n",
    "        acc = session.run(accuracy, feed_dict={x: batch_x, y: batch_y, keep_prob: 1})\n",
    "        # Calculate batch loss\n",
    "        loss = session.run(loss_f, feed_dict={x: batch_x, y: batch_y, keep_prob: 1})\n",
    "        print(\"Iter \" + str(itr) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.5f}\".format(acc))\n",
    "\n",
    "print('Test accuracy: ',round(session.run(accuracy, feed_dict={x: X_test, y: y_test, keep_prob: 1}) , 3))\n",
    "saver.save(session, save_path = \"./model/acoustic.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test new wav file\n",
    "**new_60Hz.wav** we test here is recorded with a new motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_features,_ = extract_features('./data', [\"test_new_60Hz.wav\"], [\"Unknown\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_predicts = session.run(prediction, feed_dict={x: test_features, keep_prob: 1})\n",
    "LABELS = [\"new\",\"used\",\"red\"]\n",
    "predicted_logit = stats.mode(np.argmax(y_predicts,1))[0][0]\n",
    "predicted_label = LABELS[predicted_logit]\n",
    "predicted_probability = stats.mode(np.argmax(y_predicts,1))[1][0] / len(y_predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('new', 1.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_label, predicted_probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
